# 高速公路智能车型识别与拦截系统 - 详细架构设计文档 (V5.0)
**项目名称**：Intelligent Highway Vehicle Classification & Interception System (IVCIS)
**核心技术**：STM32H7 硬件加速、多域内存管理、云边协同

---

## 1. 系统概述 (System Overview)

本系统基于 **“云-边-端”** 协同架构，旨在解决中国高速公路入口场景下的违规拦截与车型确权问题。
系统核心在于利用 **STM32H753ZI** 的高性能外设（DCMI, DMA2D, JPEG Codec, ETH）构建智能视觉网关，通过 **“行缓冲流水线”** 和 **“多域内存隔离”** 技术，在有限的片上资源（SRAM）下实现了高清图像的实时采集、本地 TinyML 唤醒分析以及云端 YOLOv4-tiny 推理的无缝衔接。

---

## 2. 核心硬件架构与资源分配 (Hardware & Resource Allocation)

### 2.1 边缘网关主控 (Edge Gateway)
*   **MCU 型号**：ST Nucleo-H753ZI (Arm Cortex-M7, 480MHz)。
*   **关键特性利用**：
    *   **DCMI**：8-14位并行摄像头接口，支持硬件同步。
    *   **Hardware JPEG Codec**：硬件级 JPEG 编解码器，解决 RGB565 数据过大的存储难题。
    *   **DMA2D (Chrom-ART)**：2D 图形加速器，用于图像格式转换与缩放，卸载 CPU 负载。
    *   **MDMA (Master DMA)**：用于不同内存域之间的高速数据搬运。

### 2.2 内存域划分策略 (SRAM Partitioning Strategy)
针对 SVGA (800x600) RGB565 图像数据量 (960KB) 超过最大连续内存块 (AXI SRAM 512KB) 的物理限制，以及 DCMI/ETH/CPU 并发导致的总线拥塞问题，系统采用严格的**多域内存隔离策略**：

| 内存域 (Domain) | 容量  | 物理位置  | 分配任务 (Allocation)                                        | 访问主控 (Masters)                    |
| :-------------- | :---- | :-------- | :----------------------------------------------------------- | :------------------------------------ |
| **D1 Domain**   | 512KB | AXI SRAM  | **1. TinyML 模型张量堆 (Arena)**<br>**2. JPEG 输出缓冲区 (约80KB)**<br>**3. 应用程序栈/堆** | CPU (Cortex-M7)<br>MDMA<br>JPEG Codec |
| **D2 Domain**   | 288KB | SRAM1/2/3 | **1. DCMI 行缓冲区 (Line Buffers)**<br>**2. 以太网描述符 (Tx/Rx Desc)**<br>**3. 以太网数据包缓冲 (PBUF)** | DCMI DMA<br>Ethernet DMA<br>DMA2D     |
| **D3 Domain**   | 64KB  | SRAM4     | **1. 系统日志缓存**<br>**2. 调试数据**                       | BDMA<br>CPU (低频访问)                |

**设计意图**：将高带宽需求的 DCMI 和 Ethernet 数据流隔离在 **D2 域**，通过 AHB 总线矩阵独立运行；将 CPU 密集型的 AI 推理隔离在 **D1 域**。两者互不干扰，彻底解决总线拥塞。

---

## 3. 关键技术实现细节 (Technical Implementation Details)

### 3.1 解决内存溢出的“分片流水线”设计 (Slice Pipeline)
由于无法存储完整的 960KB 原始图像，系统摒弃了“先存后算”的传统模式，采用 **“流式处理 (Streaming Processing)”** 架构。

#### **阶段 A：采集与分流 (Acquisition & Splitting)**
1.  **源数据**：OV5640 配置为输出 **800x600 RGB565** 连续流。
2.  **行缓冲 (Line Buffering)**：
    *   在 **D2 SRAM** 中定义双行缓冲区 `Line_Buf[2][1600 Bytes]`。
    *   配置 DCMI 的 DMA 为**循环模式**，交替填充这两行数据。
3.  **实时分流 (Real-time Forking)**：
    *   每当一行数据接收完成（触发 DMA 半传输/全传输中断），系统立即启动两条并行处理路径。

#### **阶段 B：路径一 —— 本地 TinyML 唤醒 (Local Inference)**
*   **目标**：将 800x600 缩放为 96x96 RGB888 供 AI 使用。
*   **实现**：
    *   利用 **DMA2D** 或 CPU 极简抽样算法。
    *   **垂直抽样**：每接收约 6 行数据，仅处理 1 行，丢弃其余行。
    *   **水平抽样**：在保留的那一行中，每隔约 8 个像素取 1 个点。
    *   **存储**：抽样后的像素直接写入 **D1 SRAM** 中的 `Tiny_Input_Buf`。
*   **结果**：帧结束时，内存中仅生成了一张 **18KB** 的缩略图，未占用大块内存。

#### **阶段 C：路径二 —— 云端上传准备 (Cloud Upload Prep)**
*   **目标**：将 RGB565 原始流压缩为 JPEG 文件流。
*   **实现**：
    *   利用 STM32H7 的 **Hardware JPEG Codec**。
    *   JPEG Codec 配置为 **“On-the-fly” 编码模式**。
    *   DCMI 的数据流直接通过内部总线喂给 JPEG Codec（或通过 MDMA 搬运）。
    *   Codec 输出压缩后的字节流，写入 **D1 SRAM** 的 `JPEG_Output_Buf`。
*   **结果**：帧结束时，内存中生成了一张约 **40KB-80KB** 的 JPEG 图片，完全在 512KB AXI SRAM 的承受范围内。

---

### 3.2 解决总线拥塞的并发控制 (Concurrency Control)

为了防止 DCMI（写内存）、JPEG Codec（读写内存）和 Ethernet（读内存）同时争抢总线，系统采用以下机制：

1.  **物理隔离**：如 2.2 节所述，DCMI 和 Ethernet 的缓冲区物理上位于不同的 SRAM 块（SRAM1 vs SRAM2），通过总线矩阵的不同从接口访问，物理上消除了冲突。
2.  **MDMA 桥接**：
    *   当需要将 D2 域的数据（如网络包）搬运到 D1 域时，强制使用 **MDMA**。
    *   MDMA 独立于 CPU 运行，且拥有独立的总线访问端口，不会阻塞 CPU 取指或数据访问。
3.  **优先级配置**：
    *   **DMA 优先级**：DCMI DMA > JPEG Codec > Ethernet DMA。确保图像采集绝不丢数据（Overrun），网络发送可以稍作等待。
    *   **中断优先级**：DCMI 帧中断 > JPEG 编码结束中断 > 以太网中断。

---

## 4. 软件技术栈与逻辑 (Software Stack & Logic)

### 4.1 边缘端固件 (STM32 Firmware)
*   **RTOS**：**FreeRTOS V10**。
    *   *Task 1 (High)*: **Camera_Flow**。负责管理 DCMI 事件、触发 JPEG 编码、管理行缓冲标志位。
    *   *Task 2 (Medium)*: **AI_Inference**。当 `Tiny_Input_Buf` 就绪后，运行 X-CUBE-AI 推理。
    *   *Task 3 (Low)*: **Net_Task**。负责 LwIP 协议栈维护及 HTTP 发送。
*   **AI 引擎**：**STM32Cube.AI**。
    *   *模型*：MobileNet V2 0.1 (Quantized INT8)。
    *   *推理时间*：< 50ms (利用 H7 的 DSP 指令集加速)。
*   **网络协议**：**LwIP (Netconn API)**。
    *   使用 **Zero-Copy (零拷贝)** 发送机制：直接将 `JPEG_Output_Buf` 的指针传递给 LwIP 发送函数，避免 CPU 内存拷贝带来的开销。

### 4.2 云端服务 (Cloud Backend)
*   **服务框架**：Python **FastAPI** (异步非阻塞)。
*   **AI 引擎**：**ONNX Runtime** (GPU/CPU)。
    *   *模型*：YOLOv4-tiny (Custom 8 Classes)。
    *   *输入*：608x608。
*   **图像处理**：
    *   接收 JPEG 流 -> 解码 -> **CLAHE (自适应直方图均衡)** -> 推理。
    *   *注：图像增强移至云端进行，利用服务器强大算力，进一步减轻 MCU 负担。*

---

## 5. 核心业务流程时序 (Core Workflow Timing)

1.  **T0 - 启动采集**：DCMI 开始以 800x600 RGB565 格式向 D2 SRAM 的行缓冲区写入数据。
2.  **T0 ~ T+30ms - 流水线处理**：
    *   **硬件动作**：JPEG Codec 实时读取行缓冲，编码数据写入 D1 SRAM。
    *   **硬件动作**：DMA2D 实时抽样，将缩略图写入 D1 SRAM。
3.  **T+30ms - 帧结束**：
    *   DCMI 传输完成。内存中现有一张 96x96 缩略图和一张完整 JPEG 文件。
4.  **T+35ms - 本地唤醒**：
    *   CPU 介入，运行 TinyML 推理。
    *   若判定为“无车”，直接丢弃 JPEG 缓冲区，复位状态。
5.  **T+80ms - 触发上传 (若有车)**：
    *   LwIP 协议栈接管 JPEG 缓冲区指针。
    *   Ethernet DMA 开始将数据搬运至 PHY 芯片发送。
6.  **T+500ms - 云端反馈**：
    *   接收云端 JSON 指令，执行 IO 控制。

---

## 6. 总结 (Conclusion)

本架构设计通过 **“空间换时间”**（多域内存隔离）和 **“流水线处理”**（行缓冲+实时编码）的工程手段，成功攻克了 STM32H7 在处理高清图像时的内存与带宽瓶颈。

它不仅实现了 **800x600 高清图像的零延迟采集与上传**，还保留了 **TinyML 本地智能** 的能力，是一套真正符合工业级标准、高集成度、高可靠性的嵌入式视觉网关方案。